{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2825818",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we are using the same Skeleton of chatbot and improving it by adding loops and conditions to store the chat history.\n",
    "from langgraph.graph import StateGraph,START,END\n",
    "from langchain_openai import ChatOpenAI\n",
    "from typing import TypedDict, Literal\n",
    "import operator\n",
    "from typing import List,Annotated\n",
    "from langchain_core.messages import SystemMessage,HumanMessage,BaseMessage#(BaseMessage will add the flexibility of using HM , SM, AI Message in ChatBot )\n",
    "\n",
    "from langgraph.checkpoint.memory import MemorySaver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f564d598",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph.message import add_messages\n",
    "# add_messages is the built-in function to add messages to the state in langgraph which gives more flexibility for BaseMessage, instead of using operator.add \n",
    "class ChatbotState(TypedDict):\n",
    "    messages:Annotated[List[BaseMessage],add_messages]   # this is reducer function to add all the message history into the list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e310972",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=ChatOpenAI()\n",
    "def chat_node(state:ChatbotState):\n",
    "    #take user query from the state, send it to LLM , response store to state\n",
    "    messages=state['messages']\n",
    "    response=model.invoke(messages)\n",
    "    return {'messages':[response]}  # we have to pass the response as a list because in above messages is defined as List[BaseMessage]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25cec761",
   "metadata": {},
   "outputs": [],
   "source": [
    "#add Checkpointer\n",
    "Checkpointer=MemorySaver()\n",
    "\n",
    "graph=StateGraph(ChatbotState)\n",
    "\n",
    "# add nodes now\n",
    "graph.add_node('chat_node',chat_node)\n",
    "# Now add edges\n",
    "graph.add_edge(START,'chat_node')\n",
    "graph.add_edge('chat_node',END)\n",
    "\n",
    "workflow=graph.compile(checkpointer=Checkpointer)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82056c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''initial_state={\n",
    "    'messages':[HumanMessage(content='What is the capital of India? ')]\n",
    "}       \n",
    "workflow.invoke(initial_state)['messages'][-1].content'''     # this is the normal invoke of the message and get the ai output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f17f9c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''while True:\n",
    "    user_message=input(\"TypeHere: \")\n",
    "    print(\"User:\",user_message)\n",
    "    if user_message.lower() in ['exit','quit','bye']:\n",
    "        break\n",
    "    response=workflow.invoke({'messages':[HumanMessage(content=user_message)]})\n",
    "    print('AI: ',response['messages'][-1].content)                 '''                \n",
    "# this is while loop to continously chat with AI This doesn't have any chat memory becauseof no thread and memory not included \n",
    "# so for that we have to include persistence which involves, checkpointer Memory saver , thread_id and also configurable\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1efedb77",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''thread_id = '1'\n",
    "initial_state={\n",
    "    'messages':[HumanMessage(content='What is the capital of India? ')]\n",
    "}\n",
    "config = {'configurable': {'thread_id': thread_id}}\n",
    "workflow.invoke(initial_state, config=config)['messages'][-1].content'''   \n",
    "# this is thread implementation example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "084bd1cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "thread_id='1'\n",
    "while True:\n",
    "    user_message=input(\"TypeHere: \")\n",
    "    print(\"User:\",user_message)\n",
    "    if user_message.lower() in ['exit','quit','bye']:\n",
    "        print(\"Chatbot: Goodbye!\")\n",
    "        break\n",
    "    config={'configurable':{'thread_id':thread_id}} \n",
    "    response=workflow.invoke({'messages':[HumanMessage(content=user_message)]},config=config)\n",
    "    print(\"AI:\",response['messages'][-1].content)                                             \n",
    "# this involves threadid, while loop for continous chat , normal invoke with configurable \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4379f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow.get_state(config=config) # to get all the caht history and all the messages with all the details\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a393d73",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
